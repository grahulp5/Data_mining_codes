{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07629697",
   "metadata": {},
   "source": [
    "Apriori\n",
    "We have to install another package \"mlxtend\".\n",
    "If you have installed, please ignore this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55447515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from mlxtend) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152af9ad",
   "metadata": {},
   "source": [
    "Now let's try to input the example in the lecture here.\n",
    "The data is organized as transaction record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "minsup = 0.2\n",
    "minconf = 0.6\n",
    "\n",
    "# Transaction data\n",
    "dataset = [\n",
    "    ['I1', 'I2', 'I5'],\n",
    "    ['I2', 'I4'],\n",
    "    ['I2', 'I3'],\n",
    "    ['I1', 'I2', 'I4'],\n",
    "    ['I1', 'I3'],\n",
    "    ['I2', 'I3'],\n",
    "    ['I1', 'I3'],\n",
    "    ['I1', 'I2', 'I3', 'I5'],\n",
    "    ['I1', 'I2', 'I3']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270d0e0",
   "metadata": {},
   "source": [
    "We have to transform the data into the format that apriori can recognize.\n",
    "Let's print out df and see what kind of format it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fee252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      I1     I2     I3     I4     I5\n",
      "0   True   True  False  False   True\n",
      "1  False   True  False   True  False\n",
      "2  False   True   True  False  False\n",
      "3   True   True  False   True  False\n",
      "4   True  False   True  False  False\n",
      "5  False   True   True  False  False\n",
      "6   True  False   True  False  False\n",
      "7   True   True   True  False   True\n",
      "8   True   True   True  False  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeung\\Anaconda3\\envs\\py39\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "\n",
    "# Convert the array into a pandas DataFrame\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1c103",
   "metadata": {},
   "source": [
    "Train the rules here.\n",
    "Note: the minsup and minconf are set at the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b4b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeung\\Anaconda3\\envs\\py39\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Use the apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=minsup, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=minconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857a987",
   "metadata": {},
   "source": [
    "Let's show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6c9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "     support      itemsets\n",
      "0   0.666667          (I1)\n",
      "1   0.777778          (I2)\n",
      "2   0.666667          (I3)\n",
      "3   0.222222          (I4)\n",
      "4   0.222222          (I5)\n",
      "5   0.444444      (I2, I1)\n",
      "6   0.444444      (I3, I1)\n",
      "7   0.222222      (I5, I1)\n",
      "8   0.444444      (I2, I3)\n",
      "9   0.222222      (I2, I4)\n",
      "10  0.222222      (I2, I5)\n",
      "11  0.222222  (I2, I3, I1)\n",
      "12  0.222222  (I2, I5, I1)\n",
      "\n",
      "Association Rules:\n",
      "  antecedents consequents   support  confidence      lift\n",
      "0        (I1)        (I2)  0.444444    0.666667  0.857143\n",
      "1        (I3)        (I1)  0.444444    0.666667  1.000000\n",
      "2        (I1)        (I3)  0.444444    0.666667  1.000000\n",
      "3        (I5)        (I1)  0.222222    1.000000  1.500000\n",
      "4        (I3)        (I2)  0.444444    0.666667  0.857143\n",
      "5        (I4)        (I2)  0.222222    1.000000  1.285714\n",
      "6        (I5)        (I2)  0.222222    1.000000  1.285714\n",
      "7    (I2, I5)        (I1)  0.222222    1.000000  1.500000\n",
      "8    (I5, I1)        (I2)  0.222222    1.000000  1.285714\n",
      "9        (I5)    (I2, I1)  0.222222    1.000000  2.250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeung\\Anaconda3\\envs\\py39\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Display generated rules\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9860f",
   "metadata": {},
   "source": [
    "Parameters for apriori()\n",
    "min_support: minimum support, for programming, we usually put it as a variable which set at the beginning of the program.\n",
    "\n",
    "use_colnames='True': the function uses column names in the DataFrame for the itemsets rather than column indices. By default, it's set to False.\n",
    "\n",
    "max_len: Specifies the maximum length of the itemsets generated. By default, it's exhaustly listed all itemsets.\n",
    "\n",
    "verbose: Shows the number of iterations through the loop for finding frequent itemsets. For large dataset, you can set this number such that you know the program is not hang there.\n",
    "\n",
    "low_memory='True': the algorithm will use less memory and more memory-efficient approach to finding frequent itemsets. By default, it's set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af303406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing 18 combinations | Sampling itemset size 2\n",
      "Frequent Itemsets:\n",
      "     support  itemsets\n",
      "0   0.666667      (I1)\n",
      "1   0.777778      (I2)\n",
      "2   0.666667      (I3)\n",
      "3   0.222222      (I4)\n",
      "4   0.222222      (I5)\n",
      "5   0.444444  (I2, I1)\n",
      "6   0.444444  (I3, I1)\n",
      "7   0.222222  (I5, I1)\n",
      "8   0.444444  (I2, I3)\n",
      "9   0.222222  (I2, I4)\n",
      "10  0.222222  (I2, I5)\n",
      "\n",
      "Association Rules:\n",
      "  antecedents consequents   support  confidence  lift\n",
      "0    (I2, I1)        (I5)  0.222222         0.5  2.25\n",
      "1        (I5)    (I2, I1)  0.222222         1.0  2.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeung\\Anaconda3\\envs\\py39\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#frequent_itemsets = apriori(df, min_support=minsup, use_colnames=True)\n",
    "frequent_itemsets = apriori(df, min_support=minsup, use_colnames=True, max_len=2,verbose=1,low_memory='False')\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Display generated rules\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef0816",
   "metadata": {},
   "source": [
    "Parameters for association_rules()\n",
    "\n",
    "metric:\n",
    "'support': The support of the rule in the dataset. i.e. to use support again for the association rule.\n",
    "'confidence': That is what we have discussed in the lecture.\n",
    "'lift': The lift of the rule. A lift value greater than 1 indicates that the rule's antecedent and consequent are more likely to occur together than would be expected if they were statistically independent. You can think if you want a stronger condition for association rules, you can set it as 1.5 or 2.\n",
    "There are other metrics include 'leverage' and 'conviction'.\n",
    "\n",
    "min_threshold: The minimum threshold for the given metric to consider a rule interesting.\n",
    "\n",
    "support_only: A boolean that, if True, the function will only return the support metric for the rules without calculating any other metrics like confidence or lift. This can be useful for very large datasets. The default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133321da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "     support  itemsets\n",
      "0   0.666667      (I1)\n",
      "1   0.777778      (I2)\n",
      "2   0.666667      (I3)\n",
      "3   0.222222      (I4)\n",
      "4   0.222222      (I5)\n",
      "5   0.444444  (I2, I1)\n",
      "6   0.444444  (I3, I1)\n",
      "7   0.222222  (I5, I1)\n",
      "8   0.444444  (I2, I3)\n",
      "9   0.222222  (I2, I4)\n",
      "10  0.222222  (I2, I5)\n",
      "\n",
      "Association Rules:\n",
      "  antecedents consequents   support  confidence  lift\n",
      "0        (I5)        (I1)  0.222222    1.000000   1.5\n",
      "1        (I1)        (I5)  0.222222    0.333333   1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeung\\Anaconda3\\envs\\py39\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.5)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Display generated rules\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe379dd",
   "metadata": {},
   "source": [
    "Item Item Collaborative Filtering\n",
    "First we have to use the package scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2903148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "     -------------------------------------- 772.0/772.0 kB 9.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from scikit-surprise) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from scikit-surprise) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\yeung\\anaconda3\\envs\\py39\\lib\\site-packages (from scikit-surprise) (1.6.3)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'done'\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp39-cp39-win_amd64.whl size=1141446 sha256=7982c23aa351ed91aad5561ceeff56c57acf73f32a5a8dc84635e6703e922a87\n",
      "  Stored in directory: c:\\users\\yeung\\appdata\\local\\pip\\cache\\wheels\\c6\\3a\\46\\9b17b3512bdf283c6cb84f59929cdd5199d4e754d596d22784\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ab149",
   "metadata": {},
   "source": [
    "Build a toy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb7c8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user item  rating\n",
       "0    A    1       5\n",
       "1    A    2       4\n",
       "2    A    3       4\n",
       "3    B    1       3\n",
       "4    B    3       2\n",
       "5    C    2       4\n",
       "6    C    3       5\n",
       "7    C    4       3\n",
       "8    D    1       2\n",
       "9    D    4       4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data: user, item, rating\n",
    "ratings_dict = {\n",
    "    'user': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C', 'D', 'D'],\n",
    "    'item': ['1', '2', '3', '1', '3', '2', '3', '4', '1', '4'],\n",
    "    'rating': [5, 4, 4, 3, 2, 4, 5, 3, 2, 4],\n",
    "}\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0dd54e",
   "metadata": {},
   "source": [
    "Change it into the Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d3a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Reader object. The rating_scale parameter specifies the rating scale.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load the dataset from the DataFrame\n",
    "data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ab147",
   "metadata": {},
   "source": [
    "Split Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b5eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<surprise.trainset.Trainset object at 0x0000021E5F368AF0>\n",
      "[('B', '3', 2.0), ('B', '1', 3.0), ('C', '3', 5.0)]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into the training set and the test set\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "print(trainset)\n",
    "\n",
    "print(testset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cc4e7",
   "metadata": {},
   "source": [
    "Build the model. Calculating the distance and using KNN. For KNN, there are 2 options: KNNWithMeans and KNNBasic. We choose KNNWithMeans first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2e73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNWithMeans at 0x21e5f386700>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use KNNWithMeans for item-based collaborative filtering\n",
    "# Here, we set 'user_based' to False to perform item-based collaborative filtering\n",
    "algo = KNNWithMeans(sim_options={'name': 'cosine', 'user_based': False})\n",
    "\n",
    "# Train the algorithm on the trainset\n",
    "algo.fit(trainset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb18780",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4abe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: B, Item: 3, True Rating: 2.0, Predicted Rating: 3.71\n",
      "User: B, Item: 1, True Rating: 3.0, Predicted Rating: 3.71\n",
      "User: C, Item: 3, True Rating: 5.0, Predicted Rating: 4.00\n",
      "RMSE: 1.2178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2177820811947069"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict ratings for the testset\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Iterate through the predictions and print user, item, true rating, and estimated rating\n",
    "for prediction in predictions:\n",
    "    user = prediction.uid\n",
    "    item = prediction.iid\n",
    "    true_rating = prediction.r_ui\n",
    "    predicted_rating = prediction.est\n",
    "    print(f\"User: {user}, Item: {item}, True Rating: {true_rating}, Predicted Rating: {predicted_rating:.2f}\")\n",
    "\n",
    "\n",
    "# Compute and print Root Mean Squared Error\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54f463",
   "metadata": {},
   "source": [
    "Let's see what is KNNWithMeans() now:\n",
    "k (default=40): The maximum number of neighbors to take into account for aggregation.\n",
    "\n",
    "min_k (default=1): The minimum number of neighbors to take into account for aggregation.\n",
    "\n",
    "sim_options: \n",
    "name: similarity measure ('cosine', 'msd', 'pearson', 'pearson_baseline').\n",
    "user_based: if True, indicates user-based collaborative filtering. If False, item-based collaborative filtering is used. \n",
    "min_support: The minimum number of common items (for item-based) or common users (for user-based) needed.\n",
    "shrinkage: only applicable in pearson_baseline similarity\n",
    "verbose (default=True): If True, print details\n",
    "\n",
    "It is very similar for KNNBasic(), without taking the user means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "\n",
    "# Configure the algorithm to be item-based with cosine similarity\n",
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': False,  # Item-based CF\n",
    "    'min_support': 5,\n",
    "}\n",
    "\n",
    "algo = KNNWithMeans(k=20, min_k=5, sim_options=sim_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1338",
   "metadata": {},
   "source": [
    "How about if we want to use the similarity score only, without using KNN?\n",
    "Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbb39e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Ratings Matrix:\n",
      "item    1    2    3    4\n",
      "user                    \n",
      "A     5.0  4.0  4.0  NaN\n",
      "B     3.0  NaN  2.0  NaN\n",
      "C     NaN  4.0  5.0  3.0\n",
      "D     2.0  NaN  NaN  4.0\n",
      "\n",
      "Item Similarity Matrix:\n",
      "item         1         2         3         4\n",
      "item                                        \n",
      "1     1.000000  0.573539  0.628746  0.259554\n",
      "2     0.573539  1.000000  0.948683  0.424264\n",
      "3     0.628746  0.948683  1.000000  0.447214\n",
      "4     0.259554  0.424264  0.447214  1.000000\n",
      "\n",
      "Predicted Rating for A -> 4: 4.23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot the DataFrame to create a user-item matrix\n",
    "ratings_matrix = df.pivot_table(index='user', columns='item', values='rating')\n",
    "print(\"User-Item Ratings Matrix:\")\n",
    "print(ratings_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Replace NaN values with 0s for similarity calculation\n",
    "ratings_matrix_filled = ratings_matrix.fillna(0)\n",
    "\n",
    "# Calculate cosine similarity between items\n",
    "item_similarity = cosine_similarity(ratings_matrix_filled.T)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=ratings_matrix.columns, columns=ratings_matrix.columns)\n",
    "\n",
    "print(\"\\nItem Similarity Matrix:\")\n",
    "print(item_similarity_df)\n",
    "\n",
    "def predict_rating(user, item):\n",
    "    # Check if the user has already rated the item\n",
    "    if pd.notna(ratings_matrix.loc[user, item]):\n",
    "        return ratings_matrix.loc[user, item]\n",
    "    \n",
    "    # Similarities between the target item and other items\n",
    "    sim_scores = item_similarity_df[item]\n",
    "    \n",
    "    # User's ratings for other items\n",
    "    user_ratings = ratings_matrix.loc[user]\n",
    "    \n",
    "    # Exclude items not rated by the user by setting their similarity to 0\n",
    "    idx_not_rated = user_ratings[user_ratings.isna()].index\n",
    "    sim_scores[idx_not_rated] = 0\n",
    "    \n",
    "    # Calculate the weighted average of ratings\n",
    "    if sim_scores.sum() > 0:\n",
    "        predicted_rating = np.dot(sim_scores, user_ratings.fillna(0)) / sim_scores.sum()\n",
    "    else:\n",
    "        predicted_rating = np.nan  # Unable to predict if there are no similar items rated by the user\n",
    "    \n",
    "    return predicted_rating\n",
    "\n",
    "# Example: Predict the rating for User A for Item 4\n",
    "user = 'A'\n",
    "item = '4'\n",
    "predicted_rating = predict_rating(user, item)\n",
    "print(f\"\\nPredicted Rating for {user} -> {item}: {predicted_rating:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb9e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85f105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c31d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6cd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2d717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
